{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMetr+5BnVRNYfw5iUchN5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ey-luccas/Aprendizado_m-quina-/blob/main/2%C2%BA_Bigdata(prot%C3%B3tipo).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Estrutura Geral do Sistema**\n",
        "**O sistema terá três etapas principais:**\n",
        "\n",
        "* Download e Leitura do PDF:\n",
        "  1. Baixar o PDF usando a URL fornecida.\n",
        "  2. Extrair o texto do PDF para análise.\n",
        "\n",
        "* Processamento e Criação do Dataset:\n",
        "  1. Identificar tópicos importantes relacionados ao assunto (neste caso, agricultura).\n",
        "  2. Organizar os dados em uma estrutura tabular, como um arquivo CSV.\n",
        "\n",
        "* Geração da Documentação:\n",
        "  1. Resumir o conteúdo do dataset.\n",
        "  2. Criar uma documentação simples sobre as informações disponíveis.\n",
        "\n",
        "* Ferramentas e Bibliotecas Necessárias:\n",
        "Linguagem: Python\n",
        "\n",
        "1.   requests: Para baixar o PDF.\n",
        "2.   PyPDF2 ou PyMuPDF (fitz): Para extrair o texto do PDF.\n",
        "3.   pandas: Para criar e manipular o dataset.\n",
        "4.   nltk ou spaCy: Para processamento de linguagem natural (NLP).\n",
        "5.   openpyxl: Caso queira salvar o dataset em Excel."
      ],
      "metadata": {
        "id": "3oHuX4Z5GC3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukWmdAaoFrMv",
        "outputId": "7bd9bf04-c529-4d6f-924a-8bfa5d0cbf9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá, bem-vindo ao sistema Pydataset!\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.14)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF baixado e salvo em ./livro_download/agricultura.pdf\n",
            "Dataset estruturado criado e salvo em ./livro_download/dataset_csv/dataset_agricultura.csv\n"
          ]
        }
      ],
      "source": [
        "print('Olá, bem-vindo ao sistema Pydataset!')\n",
        "\n",
        "# Instalação de dependências necessárias\n",
        "!pip install requests PyPDF2 PyMuPDF nltk\n",
        "\n",
        "!pip install nltk\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import string\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Download necessary NLTK resources for Portuguese tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Função para baixar o PDF\n",
        "def download_pdf(url, output_path):\n",
        "    response = requests.get(url)\n",
        "    with open(output_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"PDF baixado e salvo em {output_path}\")\n",
        "\n",
        "# Função para extrair texto do PDF\n",
        "def extrair_texto_pdf(caminho_pdf):\n",
        "    with open(caminho_pdf, 'rb') as file:\n",
        "        reader = PdfReader(file)\n",
        "        texto = \"\"\n",
        "        for page in reader.pages:\n",
        "            texto += page.extract_text()\n",
        "    return texto\n",
        "\n",
        "# Função para processar texto e identificar tópicos/subtópicos\n",
        "def processar_texto(texto):\n",
        "    # Tokenization with Portuguese support\n",
        "    tokens = word_tokenize(texto.lower().translate(str.maketrans(\"\", \"\", string.punctuation)), language='portuguese')\n",
        "    stopwords_list = set(stopwords.words('portuguese'))\n",
        "    filtered_tokens = [token for token in tokens if token.isalpha() and token not in stopwords_list]\n",
        "    word_counts = Counter(filtered_tokens)\n",
        "    return word_counts\n",
        "\n",
        "# Função para gerar perguntas/respostas baseadas em texto\n",
        "def gerar_perguntas_respostas(texto):\n",
        "    perguntas_respostas = [\n",
        "        {\"Pergunta\": \"Quais são os principais tipos de irrigação?\",\n",
        "         \"Resposta\": \"Os principais tipos de irrigação incluem aspersão, gotejamento e irrigação superficial.\",\n",
        "         \"Tópico\": \"Irrigação\",\n",
        "         \"Subtópico\": \"Tipos de Irrigação\",\n",
        "         \"Fonte\": \"Capítulo de Irrigação\",\n",
        "         \"Exemplo de Uso\": \"Usar irrigação por gotejamento para economizar água.\"},\n",
        "        {\"Pergunta\": \"Como aplicar fertilizantes corretamente?\",\n",
        "         \"Resposta\": \"Fertilizantes podem ser aplicados em cobertura, incorporados ao solo ou via fertirrigação, dependendo da cultura.\",\n",
        "         \"Tópico\": \"Fertilização\",\n",
        "         \"Subtópico\": \"Métodos de Aplicação\",\n",
        "         \"Fonte\": \"Capítulo de Fertilização\",\n",
        "         \"Exemplo de Uso\": \"Aplicar fertilizantes antes do plantio de soja.\"}\n",
        "    ]\n",
        "    # Adicione lógica personalizada para analisar o texto e expandir automaticamente\n",
        "    return perguntas_respostas\n",
        "\n",
        "# Função para criar o dataset\n",
        "def criar_dataset(perguntas_respostas, output_csv):\n",
        "    df = pd.DataFrame(perguntas_respostas)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Dataset estruturado criado e salvo em {output_csv}\")\n",
        "\n",
        "# URL do PDF e caminho para salvar o arquivo\n",
        "url = \"https://www.bibliotecaagptea.org.br/agricultura/agricultura_geral/livros/APOSTILA%20DE%20AGRICULTURA%20GERAL.pdf\"\n",
        "diretorio_pdf = \"./livro_download/agricultura.pdf\"\n",
        "os.makedirs(os.path.dirname(diretorio_pdf), exist_ok=True)\n",
        "\n",
        "# Baixa o PDF\n",
        "download_pdf(url, diretorio_pdf)\n",
        "\n",
        "# Extrai o texto do PDF\n",
        "texto_capturado = extrair_texto_pdf(diretorio_pdf)\n",
        "\n",
        "# Processa o texto para identificar palavras-chave\n",
        "word_counts = processar_texto(texto_capturado)\n",
        "\n",
        "# Gera perguntas e respostas automáticas\n",
        "perguntas_respostas = gerar_perguntas_respostas(texto_capturado)\n",
        "\n",
        "# Define o caminho do dataset\n",
        "diretorio_csv = \"./livro_download/dataset_csv\"\n",
        "os.makedirs(diretorio_csv, exist_ok=True)\n",
        "output_csv = os.path.join(diretorio_csv, \"dataset_agricultura.csv\")\n",
        "\n",
        "# Cria o dataset final\n",
        "criar_dataset(perguntas_respostas, output_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **cartinha chatinha**"
      ],
      "metadata": {
        "id": "Hc5gyxeClu9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Olá, bem-vindo ao sistema Pydataset!')\n",
        "\n",
        "# Instalação de dependências necessárias\n",
        "!pip install requests PyPDF2 PyMuPDF nltk\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import string\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Função para baixar o PDF\n",
        "def download_pdf(url, output_path):\n",
        "    response = requests.get(url)\n",
        "    with open(output_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"PDF baixado e salvo em {output_path}\")\n",
        "\n",
        "# Função para extrair texto do PDF\n",
        "def extrair_texto_pdf(caminho_pdf):\n",
        "    with open(caminho_pdf, 'rb') as file:\n",
        "        reader = PdfReader(file)\n",
        "        texto = \"\"\n",
        "        for page in reader.pages:\n",
        "            texto += page.extract_text()\n",
        "    return texto\n",
        "\n",
        "# Função para processar texto e identificar tópicos/subtópicos\n",
        "def processar_texto(texto):\n",
        "    # Tokeniza o texto em sentenças\n",
        "    sentencas = sent_tokenize(texto)\n",
        "    perguntas_respostas = []\n",
        "\n",
        "    # Loop para analisar cada sentença e identificar padrões\n",
        "    for sentenca in sentencas:\n",
        "        if \"irrigação\" in sentenca.lower():\n",
        "            perguntas_respostas.append({\n",
        "                \"Tópico\": \"Irrigação\",\n",
        "                \"Subtópico\": \"Tipos de Irrigação\",\n",
        "                \"Pergunta\": \"Quais são os tipos de irrigação?\",\n",
        "                \"Resposta\": sentenca.strip(),\n",
        "                \"Fonte\": \"Detectado automaticamente\",\n",
        "                \"Exemplo de Uso\": \"Usar o método adequado para economizar água.\"\n",
        "            })\n",
        "        elif \"fertilizante\" in sentenca.lower() or \"adubação\" in sentenca.lower():\n",
        "            perguntas_respostas.append({\n",
        "                \"Tópico\": \"Fertilização\",\n",
        "                \"Subtópico\": \"Métodos de Fertilização\",\n",
        "                \"Pergunta\": \"Como aplicar fertilizantes?\",\n",
        "                \"Resposta\": sentenca.strip(),\n",
        "                \"Fonte\": \"Detectado automaticamente\",\n",
        "                \"Exemplo de Uso\": \"Aplicar fertilizantes antes do plantio.\"\n",
        "            })\n",
        "        # Adicione mais condições para outros tópicos e subtópicos\n",
        "\n",
        "    return perguntas_respostas\n",
        "\n",
        "# Função para criar o dataset\n",
        "def criar_dataset(perguntas_respostas, output_csv):\n",
        "    df = pd.DataFrame(perguntas_respostas)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Dataset estruturado criado e salvo em {output_csv}\")\n",
        "\n",
        "# URL do PDF e caminho para salvar o arquivo\n",
        "url = \"https://www.bibliotecaagptea.org.br/agricultura/agricultura_geral/livros/APOSTILA%20DE%20AGRICULTURA%20GERAL.pdf\"\n",
        "diretorio_pdf = \"./livro_download/agricultura.pdf\"\n",
        "os.makedirs(os.path.dirname(diretorio_pdf), exist_ok=True)\n",
        "\n",
        "# Baixa o PDF\n",
        "download_pdf(url, diretorio_pdf)\n",
        "\n",
        "# Extrai o texto do PDF\n",
        "texto_capturado = extrair_texto_pdf(diretorio_pdf)\n",
        "\n",
        "# Processa o texto para identificar tópicos e gerar perguntas/respostas\n",
        "perguntas_respostas = processar_texto(texto_capturado)\n",
        "\n",
        "# Define o caminho do dataset\n",
        "diretorio_csv = \"./livro_download/dataset_csv\"\n",
        "os.makedirs(diretorio_csv, exist_ok=True)\n",
        "output_csv = os.path.join(diretorio_csv, \"dataset_agricultura.csv\")\n",
        "\n",
        "# Cria o dataset final\n",
        "criar_dataset(perguntas_respostas, output_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NdOGJ3yUgLgp",
        "outputId": "5e04b04b-fc84-407a-c074-5cebce857e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá, bem-vindo ao sistema Pydataset!\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.14)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF baixado e salvo em ./livro_download/agricultura.pdf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7a48f6fb0851>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Processa o texto para identificar tópicos e gerar perguntas/respostas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mperguntas_respostas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessar_texto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto_capturado\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Define o caminho do dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7a48f6fb0851>\u001b[0m in \u001b[0;36mprocessar_texto\u001b[0;34m(texto)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocessar_texto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Tokeniza o texto em sentenças\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0msentencas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mperguntas_respostas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Olá, bem-vindo ao sistema Pydataset!')\n",
        "\n",
        "# Instalação de dependências\n",
        "!pip install requests PyPDF2 nltk\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from PyPDF2 import PdfReader\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download punkt_tab for sentence tokenization\n",
        "\n",
        "# Função para baixar o PDF\n",
        "def download_pdf(url, output_path):\n",
        "    response = requests.get(url)\n",
        "    with open(output_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"PDF baixado e salvo em {output_path}\")\n",
        "\n",
        "# Função para extrair texto do PDF\n",
        "def extrair_texto_pdf(caminho_pdf):\n",
        "    with open(caminho_pdf, 'rb') as file:\n",
        "        reader = PdfReader(file)\n",
        "        texto = \"\"\n",
        "        for page in reader.pages:\n",
        "            texto += page.extract_text()\n",
        "    return texto\n",
        "\n",
        "# Função para processar texto e gerar perguntas/respostas\n",
        "def processar_texto_para_dataset(texto, topicos_subtopicos):\n",
        "    sentencas = sent_tokenize(texto.lower())  # Divide o texto em sentenças  # Divide o texto em sentenças\n",
        "    perguntas_respostas = []\n",
        "\n",
        "    for topico, subtopicos in topicos_subtopicos.items():\n",
        "        for subtopico, palavras_chave in subtopicos.items():\n",
        "            for palavra in palavras_chave:\n",
        "                for sentenca in sentencas:\n",
        "                    if palavra in sentenca:\n",
        "                        perguntas_respostas.append({\n",
        "                            \"Tópico\": topico,\n",
        "                            \"Subtópico\": subtopico,\n",
        "                            \"Pergunta\": f\"O que é {subtopico}?\",\n",
        "                            \"Resposta\": sentenca.strip(),\n",
        "                            \"Fonte\": \"Detectado automaticamente\",\n",
        "                            \"Exemplo de Uso\": f\"Como aplicar {subtopico} em situações práticas.\"\n",
        "                        })\n",
        "    return perguntas_respostas\n",
        "\n",
        "# Tópicos, Subtópicos e Palavras-chave\n",
        "topicos_subtopicos = {\n",
        "    \"Irrigação\": {\n",
        "        \"Tipos de Irrigação\": [\"aspersão\", \"gotejamento\", \"superficial\"],\n",
        "        \"Planejamento da Irrigação\": [\"necessidade hídrica\", \"estresse hídrico\", \"evapotranspiração\"],\n",
        "        \"Efeitos no Solo\": [\"salinidade\", \"erosão\"]\n",
        "    },\n",
        "    \"Fertilização\": {\n",
        "        \"Tipos de Fertilizantes\": [\"orgânicos\", \"minerais\", \"líquidos\"],\n",
        "        \"Correção do Solo\": [\"calagem\", \"acidez\", \"nutrientes\"],\n",
        "        \"Métodos de Aplicação\": [\"cobertura\", \"preparo do solo\", \"fertirrigação\"]\n",
        "    },\n",
        "    \"Plantio\": {\n",
        "        \"Tipos de Plantio\": [\"direto\", \"convencional\", \"cultivo mínimo\"],\n",
        "        \"Época de Plantio\": [\"zoneamento agrícola\", \"clima\", \"fotoperíodo\"],\n",
        "        \"Técnicas de Semeadura\": [\"densidade de semeadura\", \"espaçamento\", \"tratamento de sementes\"]\n",
        "    },\n",
        "    \"Colheita\": {\n",
        "        \"Métodos de Colheita\": [\"manual\", \"mecanizada\"],\n",
        "        \"Determinação do Ponto de Colheita\": [\"teor de açúcar\", \"peso\", \"umidade\"],\n",
        "        \"Pós-Colheita\": [\"secagem\", \"armazenagem\", \"beneficiamento\"]\n",
        "    },\n",
        "    \"Conservação do Solo\": {\n",
        "        \"Práticas Conservacionistas\": [\"terraceamento\", \"rotação de culturas\", \"cobertura vegetal\"],\n",
        "        \"Controle de Erosão\": [\"erosão hídrica\", \"barreiras naturais\"]\n",
        "    },\n",
        "    \"Introdução\": {\n",
        "        \"Preparo do Solo\": [\"inicial\", \"periódico\"],\n",
        "        \"Conservação do Solo\": [\"controle de erosão\", \"compactação\"],\n",
        "        \"Plantio e Semeadura\": [\"zoneamento agrícola\", \"tipos de plantio\"],\n",
        "        \"Fixação do Nitrogênio\": [\"inoculação\"],\n",
        "        \"Práticas Culturais\": [\"escarificação\", \"adubação\", \"plantas daninhas\"],\n",
        "        \"Armazenamento\": [\"fatores climáticos\", \"tipos de armazenagem\"],\n",
        "        \"Adubação Verde\": [\"vantagens\", \"plantas utilizadas\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Função para criar o dataset\n",
        "def criar_dataset(perguntas_respostas, output_csv):\n",
        "    df = pd.DataFrame(perguntas_respostas)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Dataset estruturado criado e salvo em {output_csv}\")\n",
        "\n",
        "# URL do PDF e caminho para salvar o arquivo\n",
        "url = \"https://www.bibliotecaagptea.org.br/agricultura/agricultura_geral/livros/APOSTILA%20DE%20AGRICULTURA%20GERAL.pdf\"\n",
        "diretorio_pdf = \"./livro_download/agricultura.pdf\"\n",
        "os.makedirs(os.path.dirname(diretorio_pdf), exist_ok=True)\n",
        "\n",
        "# Baixa o PDF\n",
        "download_pdf(url, diretorio_pdf)\n",
        "\n",
        "# Extrai o texto do PDF\n",
        "texto_capturado = extrair_texto_pdf(diretorio_pdf)\n",
        "\n",
        "# Processa o texto e gera perguntas/respostas\n",
        "perguntas_respostas = processar_texto_para_dataset(texto_capturado, topicos_subtopicos)\n",
        "\n",
        "# Define o caminho do dataset\n",
        "diretorio_csv = \"./livro_download/dataset_csv\"\n",
        "os.makedirs(diretorio_csv, exist_ok=True)\n",
        "output_csv = os.path.join(diretorio_csv, \"dataset_agricultura_completo.csv\")\n",
        "\n",
        "# Cria o dataset final\n",
        "criar_dataset(perguntas_respostas, output_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaLRv-AGh-49",
        "outputId": "50560da2-8b83-48c1-e3da-c5db38a5b2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá, bem-vindo ao sistema Pydataset!\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF baixado e salvo em ./livro_download/agricultura.pdf\n",
            "Dataset estruturado criado e salvo em ./livro_download/dataset_csv/dataset_agricultura_completo.csv\n"
          ]
        }
      ]
    }
  ]
}